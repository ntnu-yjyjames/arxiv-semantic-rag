from pydantic import BaseModel, Field
from typing import List, Literal, Optional, Dict, Any

# ==========================================
# üîç Search DTOs
# ==========================================
class SearchRequest(BaseModel):
    """
    Request payload for semantic vector search.
    Aggregates multiple paper titles to form a user interest vector.
    """
    titles: List[str] = Field(
        ..., 
        description="List of paper titles the user likes. Used to compute the query vector.",
        min_length=1,
        examples=[["Attention Is All You Need", "BERT: Pre-training of Deep Bidirectional Transformers"]]
    )
    backend: Literal["faiss_flat", "faiss_hnsw", "baseline"] = Field(
        "faiss_flat", 
        description="The search engine strategy. 'faiss_hnsw' is recommended for production."
    )
    top_k: int = Field(
        10, 
        ge=1, le=100, 
        description="Number of nearest neighbor chunks to retrieve."
    )
    ef_search: Optional[int] = Field(
        None, 
        description="(HNSW Only) Search depth. Higher values increase recall at the cost of latency. If None, the global HNSW_EF_SEARCH value from config is used (64 by default).‚Äù"
    )

class SearchResponse(BaseModel):
    """
    Standard search response wrapper including performance telemetry.
    """
    backend: str = Field(..., description="The backend strategy used (e.g., 'faiss_hnsw').")
    elapsed_ms: float = Field(..., description="Total execution time in milliseconds.")
    memory_rss_mb: float = Field(..., description="Current process memory usage (RSS).")
    memory_delta_mb: float = Field(..., description="Memory spike observed during this request.")
    results: List[Dict[str, Any]] = Field(..., description="List of retrieved chunks with metadata.")



# ==========================================
# üìä Benchmark DTOs
# ==========================================
class BenchmarkRequest(BaseModel):
    titles: List[str] = Field(..., min_length=1, description="Query titles.")
    top_k: int = Field(10, description="Top-K results to compare across backends.")

class BackendBenchmark(BaseModel):
    """
    Performance metrics for a single backend strategy.
    """
    backend: str
    elapsed_ms: float
    memory_rss_mb: float
    memory_delta_mb: float
    recall_at_k: float = Field(..., description="Recall@k compared to the exact Flat index (1.0 for Flat by definition).")
    results: List[Dict[str, Any]]

class BenchmarkResponse(BaseModel):
    top_k: int
    backends: List[BackendBenchmark] = Field(..., description="Comparative metrics for Flat, HNSW, and Baseline.")


# ==========================================
# ü§ñ RAG DTOs
# ==========================================
class RagRequest(BaseModel):
    question: str = Field(..., description="User's natural language query.", min_length=3)
    top_k: int = Field(10, description="Number of context chunks to retrieve for the LLM.")
    max_tokens: int = Field(512, description="Maximum token limit for the generated answer.")

class RagResponse(BaseModel):
    question: str
    top_k: int
    answer: str = Field(..., description="The synthesized answer generated by the LLM.")
    contexts: List[Dict[str, Any]] = Field(
        ..., 
        description="The actual text chunks used as evidence (Grounding) for the answer."
    )


# ==========================================
# üìö Recommendation DTOs
# ==========================================
class RecommendRequest(BaseModel):
    titles: List[str] = Field(...,min_length=1, description="User history/liked papers.")
    backend: Literal["faiss_flat", "faiss_hnsw", "baseline"] = "faiss_flat"
    top_k_docs: int = Field(10, description="Number of unique papers (not chunks) to recommend.")

class RecommendItem(BaseModel):
    doc_idx: int
    title: str
    score: float = Field(..., description="Aggregated relevance score.")

class RecommendResponse(BaseModel):
    backend: str
    results: List[RecommendItem]